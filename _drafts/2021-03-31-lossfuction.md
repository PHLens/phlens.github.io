---
layout: single
title: "SoftMax和CrossEntropy"
toc: true
toc_label: "目录"
toc_icon: "bars"
category: 深度学习基础
date: 2021-03-32 10:00+0000
typora-root-url: ..\assets\images
---

#### 二分类问题

对于二分类问题，常见的算法是逻辑回归，逻辑回归使用Sigmoid激活函数来将输出映射到$[0,1]$之间，以表示某类事件发生的概率。
$$
S(x)=\frac{1}{1+e^{-x}}
$$
![Sigmoid 曲线](/format,f_auto)

使用逻辑回归算法预测二分类问题时，可以在输出层设置一个节点，用来表示某个事件A发生的概率$P(A|x)$；也可以设置两个节点来表示事件$P(A|x)$和$P(\bar{A}|x)$发生的概率，满足条件$P(A|x)+P(\bar{A}|x)=1$，类似地也可以推广到n个输出节点。这样相比二分类问题就多出了个约束条件：概率相加为1。于是我们就希望有一种函数不仅可以将输出映射到$[0,1]$并且自然地满足概率相加为1的条件，这个函数就是SoftMax函数。



#### SoftMax函数

SoftMax的含义在于，它不是唯一的确定一个max值，而是通过给定概率来表示每个输出类别发生的可能性，概率大的类别更可能发生，但并不代表概率小的类别一定不会发生。其数学定义如下：
$$
\operatorname{Softmax}\left(z_{i}\right)=\frac{e^{z_{i}}}{\sum_{c=1}^{C} e^{z_{c}}}
$$
其中$z_i$为第$i$个节点的输出值，C为输出类别数，通过上式就能将输出限制在$[0,1]$之间，且满足概率和为1.

#### SoftMax的优缺点

- 优点：
  - 通过引入指数函数，可以使得输出对输入的敏感度增强，可以快速的拉开不同类别之间的距离
  - 指数函数求导方便
- 缺点：
  - 指数爆炸容易造成参数溢出

#### CrossEntropy交叉熵损失

SoftMax通常和交叉熵损失结合使用，

