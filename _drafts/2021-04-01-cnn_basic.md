---
layout: single
title: "CNN基础"
toc: true
toc_label: "目录"
toc_icon: "bars"
category: 计算机视觉
date: 2021-04-01 16:00+0000
typora-root-url: ../../../blog
---

#### 全连接网络的问题

> - 将输入展开为向量的形式，丢失了数据的空间性质
> - 参数量过大，训练困难
> - 容易造成过拟合



#### 卷积神经网络的核心思想

1. 局部连接
    - 每次对输入数据的一个局部区域进行操作
    - “局部”是指空间层（W$\times$ H）在深度层，即通道上，对每个通道都进行处理
2. 权值共享
    - 通常来说每个通道上的卷积核参数是不一样的，但是可以使每个通道共享一个卷积核的参数来达到减少参数量的目的
    - 当每个通道共享一个权重矩阵时，对输入图片的操作可以看成是权重矩阵对输入图片的卷积。（卷积神经网络的来源）
    - 若图片明显具有多样的特征，在不同方向上特征不同，则还是通过设置不同的卷积核来提取特征更好。
3. 下采样（池化层）

#### 卷积神经网络的结构

- 卷积层

    - 卷积核（filter）

        > size：W$\times$H（感受野）
        >
        > - 每个卷积核负责感知某一类型的特征，例如垂直特征，水平特征
        >
        > - 通过设置多个卷积核来感知多种不同的特征
        > - 普通卷积层用卷积核对输入数据的每个通道都进行卷积操作最后结果相加

    - 计算feature map

        > 三个超参数决定feature map的大小：卷积核数量F（depth）、卷积核移动的步长S（stride）、填充P（padding）
        >
        > 设输入大小为W$\times$W，则输出的feature map的大小为：$(W-F+2P)/S + 1$
        >
        > 通过控制padding可以控制输出的feature map大小与输入大小一致（same）：
        >
        > ​	一般而言，在$S=1$的情况下，设置$P=(F-1)/2$即可保持输出与输入大小一致
        >
        > feature map的大小必须为整数，因此必须合理设计$P$和$S$.
        >
        > padding的好处除此之外还有**保证数据边缘的信息不会丢失太多**.
        >
        > ***tips：***
        >
        > 使用两个$3\times3$的卷积核代替一个$5\times5$可以达到一样的输出(保持feature map不变)，但是：
        >
        > - 参数量可以减少（$3\times3\times2 \rightarrow 5\times5$）
        > - 具有更好的非线性，每个卷积层后都接一个激活层，使网络具有更好的判别性

    - 卷积操作的矩阵实现

        > - 对输入图片在每个filter区域进行im2col操作，转换成矩阵的一列，整个input转换成一个大的矩阵X_col，相应地将卷积核也进行im2col操作，转换成一个大的矩阵W_row 
        > - 然后利用高效的矩阵乘法运算计算结果矩阵
        > - 输出再reshape到feature map的尺寸

    - 特殊类型的卷积

        > 1. $1\times1$卷积：
        >     - 用于在保持feature map大小不变的情况下进行降维或升维，通过控制卷积核数量即可；
        >     - 同时也是高效**整合通道信息**的手段
        >     - 计算起来效率也更高，因为普通的卷积需要进行im2col操作， 由于输入数据空间上不具有局部性特点，大量时间花在访问内存上，而$1\times1$卷积不需要进行im2col操作，大大提高了计算效率
        >
        > 2. 空洞卷积（dilation）
        >     - 在卷积核中填充0，以扩大感受野并且降低计算量
        >     - 例如：用填充一个0的$3\times3$卷积核可以达到$5\times5$的感受野，但参数量更少
        >     - 存在的问题：
        >         - 局部信息丢失：填充0后，卷积核操作的数据可能不再具有相关性，远距离的输入信号相关性更低，因此丢失了信息，影响分类结果

- 激活层（ReLU，Sigmoid）

- 池化层

    - 池化层的基本特点：

    > - 池化层的作用：
    >     - 减少参数数量，防止过拟合
    >     - 压缩feature map，提高计算速度
    > - 池化通常设置$2\times2$的卷积核以$S=2$的步长操作输入数据，对每个通道单独操作，保持通道维度不变
    > - 一般不使用padding
    > - 常用的池化层卷积核设置 F=3,S=2 (also called overlapping pooling), and more commonly F=2,S=2. 
    > - 由于池化层激活函数简单，一般为Maxpool，无需更新参数，可以使用卷积层代替池化层以学习更多复杂的特征函数（learnable pooling layer）

    - Global pooling

    > 全局池化层用于代替最后的全连接层，以达到降维同时降低参数量的目的。
    >
    > 基本思想：全连接层是把卷积层的输出展开成一维，然后预测输出到一个多类别；全局池化则在卷积层输出的feature map每个通道上直接池化输出一个类别信息。

- FC层



#### 分组卷积



#### 深度可分离卷积

先进行深度卷积再进行$1\times1$卷积。

深度卷积：

- 卷积核分成与输入数据相同的多个通道部分，每个通道单独进行计算，最后得到与输入数据相同通道数的feature map
- 无法增加通道数，所以信息获取不够，因此需要后续的$1\times1$卷积来进行信息扩充

