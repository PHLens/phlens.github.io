---
layout: single
toc: true
toc_label: "目录"
toc_icon: "bars"
tag: 深度学习基础
title:	"梯度消失、梯度爆炸及过拟合、欠拟合"
date:   2021-03-14 15:00:00 +0000
---

#### 训练误差和泛化误差

有时候模型在训练集上准确率很高，但在测试集上准确率却很低，这就需要区分训练误差和泛化误差。

- **训练误差（training error）**：模型在训练集上表现出的误差；
- **泛化误差（generalization error）**：模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似

> 由于模型的参数是通过在训练数据集上训练模型而学习出的，参数的选择依据了最小化训练误差，因此训练误差的期望小于或等于泛化误差，而由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低

*模型更应该关注降低**泛化误差**。*



#### 过拟合和欠拟合

模型训练中经常出现的两类典型问题：一类是模型无法得到较低的训练误差，这一现象称作欠拟合（underfitting）；另一类是模型的训练误差远小于它在测试数据集上的误差，该现象称为过拟合。

*影响模型拟合问题的因素：*

- **模型的复杂度**：给定训练数据集，模型复杂度过低就会造成欠拟合，而模型复杂度太高又会导致过拟合。
- **训练数据集**：训练数据集中样本太少就会容易造成过拟合。

从造成拟合问题出现的因素出发，可以有几种**解决办法**：

1. 从模型复杂度出发：
    - ***正则化***：通过对Loss函数增加正则化项来抑制模型中部分权重的大小，一定程度上降低模型复杂度，从而避免过拟合的出现
    - ***Dropout***：对神经网络中的节点按一定的概率随机将其权重置为0，达到对于每个mini-batch都是在训练不同网络的效果,防止过拟合，本质可以看成是一种正则化
    - ***early stop***：为了防止模型对训练数据过度拟合，可以提前终止模型训练 
    - ***模型集成***：通过训练多个不同结构的模型，然后按一定权重综合所有的模型，可以增强模型的泛化能力（bagging，boosting）
2. 从训练数据集出发：
    - ***增加训练数据量***：最简单的方法就是增加训练的数据量，但很多时候往往代价高昂
    - ***交叉验证***：通过将训练数据划分为多个fold，每次选择一个来作为验证集，多次训练以提高模型的泛化能力
    - ***数据增广***：例如对于图片数据，可以通过旋转，裁剪，缩放等操作人为增加数据量



#### 梯度消失和爆炸

当神经网络层数很多时，模型的数值稳定性变的很差，对于输入变得很敏感，容易出现梯度消失和梯度爆炸的现象，造成模型训练困难。

一些常见的解决办法：

- ***随机初始化***：按某种概率分布随机初始化模型参数
- ***选择合适的激活函数***：Sigmoid函数在正负无穷方向梯度近乎为零，随着模型训练的进行，模型参数越来越趋近于激活函数上下限，梯度几乎消失，模型收敛但很多时候loss还是很大；使用ReLU激活函数可以避免出现这种问题
- ***Batch Normalization（BN）***：对每个Batch的训练数据都进行标准化，使其服从某种分布，从而可以将激活函数的输入值落在比较敏感的位置，比如Sigmoid的小数值的部分
- ***Residual Block***：随着网络层数的加深，特征被靠近输出层的layer提取的差不多了，网络的梯度变得很小，模型认为已经收敛，通过添加一条shortcut连接到之前的层，从而使梯度变大，网络能够继续训练

> ***Sigmoid 和 ReLU的区别***：
>
> Sigmoid的导数只有在0的附近时有较好的激活性,而在正负饱和区域的梯度趋向于0,从而产生梯度弥散的现象,而ReLU在大于0的部分梯度为常数,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。

